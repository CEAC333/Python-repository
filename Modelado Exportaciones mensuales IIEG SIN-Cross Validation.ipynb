{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor #modelo random forest\n",
    "from sklearn.svm import SVR #support vector regression\n",
    "from sklearn.linear_model import Ridge,Lasso # regression with regularization: Rigde: is given by the l2-norm. Also known as Ridge Regression \n",
    "# or Tikhonov regularization ///Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
    "from sklearn.kernel_ridge import KernelRidge #kernel ridge regression\n",
    "from sklearn import preprocessing # para feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score #métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pais_comprador</th>\n",
       "      <th>Industria</th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Monto</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>62364404.0</td>\n",
       "      <td>67485328.0</td>\n",
       "      <td>68750896.0</td>\n",
       "      <td>53314410.0</td>\n",
       "      <td>45624927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>8</td>\n",
       "      <td>60669012.0</td>\n",
       "      <td>62364404.0</td>\n",
       "      <td>67485328.0</td>\n",
       "      <td>68750896.0</td>\n",
       "      <td>38402717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>65432673.0</td>\n",
       "      <td>60669012.0</td>\n",
       "      <td>62364404.0</td>\n",
       "      <td>67485328.0</td>\n",
       "      <td>45714515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>10</td>\n",
       "      <td>55449279.0</td>\n",
       "      <td>65432673.0</td>\n",
       "      <td>60669012.0</td>\n",
       "      <td>62364404.0</td>\n",
       "      <td>53314410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>64030031.0</td>\n",
       "      <td>55449279.0</td>\n",
       "      <td>65432673.0</td>\n",
       "      <td>60669012.0</td>\n",
       "      <td>68750896.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>69420372.0</td>\n",
       "      <td>64030031.0</td>\n",
       "      <td>55449279.0</td>\n",
       "      <td>65432673.0</td>\n",
       "      <td>67485328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60262319.0</td>\n",
       "      <td>69420372.0</td>\n",
       "      <td>64030031.0</td>\n",
       "      <td>55449279.0</td>\n",
       "      <td>62364404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2</td>\n",
       "      <td>67522980.0</td>\n",
       "      <td>60262319.0</td>\n",
       "      <td>69420372.0</td>\n",
       "      <td>64030031.0</td>\n",
       "      <td>60669012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>3</td>\n",
       "      <td>86290892.0</td>\n",
       "      <td>67522980.0</td>\n",
       "      <td>60262319.0</td>\n",
       "      <td>69420372.0</td>\n",
       "      <td>65432673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>4</td>\n",
       "      <td>77937911.0</td>\n",
       "      <td>86290892.0</td>\n",
       "      <td>67522980.0</td>\n",
       "      <td>60262319.0</td>\n",
       "      <td>55449279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>5</td>\n",
       "      <td>85108588.0</td>\n",
       "      <td>77937911.0</td>\n",
       "      <td>86290892.0</td>\n",
       "      <td>67522980.0</td>\n",
       "      <td>64030031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>6</td>\n",
       "      <td>73835368.0</td>\n",
       "      <td>85108588.0</td>\n",
       "      <td>77937911.0</td>\n",
       "      <td>86290892.0</td>\n",
       "      <td>69420372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>7</td>\n",
       "      <td>78546337.0</td>\n",
       "      <td>73835368.0</td>\n",
       "      <td>85108588.0</td>\n",
       "      <td>77937911.0</td>\n",
       "      <td>60262319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>8</td>\n",
       "      <td>69221493.0</td>\n",
       "      <td>78546337.0</td>\n",
       "      <td>73835368.0</td>\n",
       "      <td>85108588.0</td>\n",
       "      <td>67522980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>9</td>\n",
       "      <td>84228644.0</td>\n",
       "      <td>69221493.0</td>\n",
       "      <td>78546337.0</td>\n",
       "      <td>73835368.0</td>\n",
       "      <td>86290892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>10</td>\n",
       "      <td>82759261.0</td>\n",
       "      <td>84228644.0</td>\n",
       "      <td>69221493.0</td>\n",
       "      <td>78546337.0</td>\n",
       "      <td>77937911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>11</td>\n",
       "      <td>71924711.0</td>\n",
       "      <td>82759261.0</td>\n",
       "      <td>84228644.0</td>\n",
       "      <td>69221493.0</td>\n",
       "      <td>85108588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>12</td>\n",
       "      <td>70730651.0</td>\n",
       "      <td>71924711.0</td>\n",
       "      <td>82759261.0</td>\n",
       "      <td>84228644.0</td>\n",
       "      <td>73835368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62023950.0</td>\n",
       "      <td>70730651.0</td>\n",
       "      <td>71924711.0</td>\n",
       "      <td>82759261.0</td>\n",
       "      <td>78546337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2</td>\n",
       "      <td>62459404.0</td>\n",
       "      <td>62023950.0</td>\n",
       "      <td>70730651.0</td>\n",
       "      <td>71924711.0</td>\n",
       "      <td>69221493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>3</td>\n",
       "      <td>73626985.0</td>\n",
       "      <td>62459404.0</td>\n",
       "      <td>62023950.0</td>\n",
       "      <td>70730651.0</td>\n",
       "      <td>84228644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>4</td>\n",
       "      <td>69694547.0</td>\n",
       "      <td>73626985.0</td>\n",
       "      <td>62459404.0</td>\n",
       "      <td>62023950.0</td>\n",
       "      <td>82759261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>5</td>\n",
       "      <td>70193502.0</td>\n",
       "      <td>69694547.0</td>\n",
       "      <td>73626985.0</td>\n",
       "      <td>62459404.0</td>\n",
       "      <td>71924711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>6</td>\n",
       "      <td>65390258.0</td>\n",
       "      <td>70193502.0</td>\n",
       "      <td>69694547.0</td>\n",
       "      <td>73626985.0</td>\n",
       "      <td>70730651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>7</td>\n",
       "      <td>84103970.0</td>\n",
       "      <td>65390258.0</td>\n",
       "      <td>70193502.0</td>\n",
       "      <td>69694547.0</td>\n",
       "      <td>62023950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>8</td>\n",
       "      <td>91348734.0</td>\n",
       "      <td>84103970.0</td>\n",
       "      <td>65390258.0</td>\n",
       "      <td>70193502.0</td>\n",
       "      <td>62459404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>9</td>\n",
       "      <td>89503618.0</td>\n",
       "      <td>91348734.0</td>\n",
       "      <td>84103970.0</td>\n",
       "      <td>65390258.0</td>\n",
       "      <td>73626985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>10</td>\n",
       "      <td>86753087.0</td>\n",
       "      <td>89503618.0</td>\n",
       "      <td>91348734.0</td>\n",
       "      <td>84103970.0</td>\n",
       "      <td>69694547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>11</td>\n",
       "      <td>64925972.0</td>\n",
       "      <td>86753087.0</td>\n",
       "      <td>89503618.0</td>\n",
       "      <td>91348734.0</td>\n",
       "      <td>70193502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.353150e+07</td>\n",
       "      <td>2.954424e+08</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>12</td>\n",
       "      <td>84419002.0</td>\n",
       "      <td>64925972.0</td>\n",
       "      <td>86753087.0</td>\n",
       "      <td>89503618.0</td>\n",
       "      <td>65390258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7360</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7361</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7362</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7363</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7377</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7379</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7380</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7383</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>1.397137e+05</td>\n",
       "      <td>9.837687e+03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7379 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pais_comprador     Industria     Año  Mes       Monto         t-1  \\\n",
       "6       3.353150e+07  2.954424e+08  2000.0    7  62364404.0  67485328.0   \n",
       "7       3.353150e+07  2.954424e+08  2000.0    8  60669012.0  62364404.0   \n",
       "8       3.353150e+07  2.954424e+08  2000.0    9  65432673.0  60669012.0   \n",
       "9       3.353150e+07  2.954424e+08  2000.0   10  55449279.0  65432673.0   \n",
       "10      3.353150e+07  2.954424e+08  2000.0   11  64030031.0  55449279.0   \n",
       "11      3.353150e+07  2.954424e+08  2000.0   12  69420372.0  64030031.0   \n",
       "12      3.353150e+07  2.954424e+08  2001.0    1  60262319.0  69420372.0   \n",
       "13      3.353150e+07  2.954424e+08  2001.0    2  67522980.0  60262319.0   \n",
       "14      3.353150e+07  2.954424e+08  2001.0    3  86290892.0  67522980.0   \n",
       "15      3.353150e+07  2.954424e+08  2001.0    4  77937911.0  86290892.0   \n",
       "16      3.353150e+07  2.954424e+08  2001.0    5  85108588.0  77937911.0   \n",
       "17      3.353150e+07  2.954424e+08  2001.0    6  73835368.0  85108588.0   \n",
       "18      3.353150e+07  2.954424e+08  2001.0    7  78546337.0  73835368.0   \n",
       "19      3.353150e+07  2.954424e+08  2001.0    8  69221493.0  78546337.0   \n",
       "20      3.353150e+07  2.954424e+08  2001.0    9  84228644.0  69221493.0   \n",
       "21      3.353150e+07  2.954424e+08  2001.0   10  82759261.0  84228644.0   \n",
       "22      3.353150e+07  2.954424e+08  2001.0   11  71924711.0  82759261.0   \n",
       "23      3.353150e+07  2.954424e+08  2001.0   12  70730651.0  71924711.0   \n",
       "24      3.353150e+07  2.954424e+08  2002.0    1  62023950.0  70730651.0   \n",
       "25      3.353150e+07  2.954424e+08  2002.0    2  62459404.0  62023950.0   \n",
       "26      3.353150e+07  2.954424e+08  2002.0    3  73626985.0  62459404.0   \n",
       "27      3.353150e+07  2.954424e+08  2002.0    4  69694547.0  73626985.0   \n",
       "28      3.353150e+07  2.954424e+08  2002.0    5  70193502.0  69694547.0   \n",
       "29      3.353150e+07  2.954424e+08  2002.0    6  65390258.0  70193502.0   \n",
       "30      3.353150e+07  2.954424e+08  2002.0    7  84103970.0  65390258.0   \n",
       "31      3.353150e+07  2.954424e+08  2002.0    8  91348734.0  84103970.0   \n",
       "32      3.353150e+07  2.954424e+08  2002.0    9  89503618.0  91348734.0   \n",
       "33      3.353150e+07  2.954424e+08  2002.0   10  86753087.0  89503618.0   \n",
       "34      3.353150e+07  2.954424e+08  2002.0   11  64925972.0  86753087.0   \n",
       "35      3.353150e+07  2.954424e+08  2002.0   12  84419002.0  64925972.0   \n",
       "...              ...           ...     ...  ...         ...         ...   \n",
       "7355    1.397137e+05  9.837687e+03  2015.0    2         0.0         0.0   \n",
       "7356    1.397137e+05  9.837687e+03  2015.0    3         0.0         0.0   \n",
       "7357    1.397137e+05  9.837687e+03  2015.0    4         0.0         0.0   \n",
       "7358    1.397137e+05  9.837687e+03  2015.0    5         0.0         0.0   \n",
       "7359    1.397137e+05  9.837687e+03  2015.0    6         0.0         0.0   \n",
       "7360    1.397137e+05  9.837687e+03  2015.0    7         0.0         0.0   \n",
       "7361    1.397137e+05  9.837687e+03  2015.0    8         0.0         0.0   \n",
       "7362    1.397137e+05  9.837687e+03  2015.0    9         0.0         0.0   \n",
       "7363    1.397137e+05  9.837687e+03  2015.0   10         0.0         0.0   \n",
       "7364    1.397137e+05  9.837687e+03  2015.0   11         0.0         0.0   \n",
       "7365    1.397137e+05  9.837687e+03  2015.0   12         0.0         0.0   \n",
       "7366    1.397137e+05  9.837687e+03  2016.0    1         0.0         0.0   \n",
       "7367    1.397137e+05  9.837687e+03  2016.0    2         0.0         0.0   \n",
       "7368    1.397137e+05  9.837687e+03  2016.0    3         0.0         0.0   \n",
       "7369    1.397137e+05  9.837687e+03  2016.0    4         0.0         0.0   \n",
       "7370    1.397137e+05  9.837687e+03  2016.0    5         0.0         0.0   \n",
       "7371    1.397137e+05  9.837687e+03  2016.0    6         0.0         0.0   \n",
       "7372    1.397137e+05  9.837687e+03  2016.0    7         0.0         0.0   \n",
       "7373    1.397137e+05  9.837687e+03  2016.0    8         0.0         0.0   \n",
       "7374    1.397137e+05  9.837687e+03  2016.0    9         0.0         0.0   \n",
       "7375    1.397137e+05  9.837687e+03  2016.0   10         0.0         0.0   \n",
       "7376    1.397137e+05  9.837687e+03  2016.0   11         0.0         0.0   \n",
       "7377    1.397137e+05  9.837687e+03  2016.0   12         0.0         0.0   \n",
       "7378    1.397137e+05  9.837687e+03  2017.0    1         0.0         0.0   \n",
       "7379    1.397137e+05  9.837687e+03  2017.0    2         0.0         0.0   \n",
       "7380    1.397137e+05  9.837687e+03  2017.0    3         0.0         0.0   \n",
       "7381    1.397137e+05  9.837687e+03  2017.0    4         0.0         0.0   \n",
       "7382    1.397137e+05  9.837687e+03  2017.0    5         0.0         0.0   \n",
       "7383    1.397137e+05  9.837687e+03  2017.0    6         0.0         0.0   \n",
       "7384    1.397137e+05  9.837687e+03  2017.0    7         0.0         0.0   \n",
       "\n",
       "             t-2         t-3         t-6  \n",
       "6     68750896.0  53314410.0  45624927.0  \n",
       "7     67485328.0  68750896.0  38402717.0  \n",
       "8     62364404.0  67485328.0  45714515.0  \n",
       "9     60669012.0  62364404.0  53314410.0  \n",
       "10    65432673.0  60669012.0  68750896.0  \n",
       "11    55449279.0  65432673.0  67485328.0  \n",
       "12    64030031.0  55449279.0  62364404.0  \n",
       "13    69420372.0  64030031.0  60669012.0  \n",
       "14    60262319.0  69420372.0  65432673.0  \n",
       "15    67522980.0  60262319.0  55449279.0  \n",
       "16    86290892.0  67522980.0  64030031.0  \n",
       "17    77937911.0  86290892.0  69420372.0  \n",
       "18    85108588.0  77937911.0  60262319.0  \n",
       "19    73835368.0  85108588.0  67522980.0  \n",
       "20    78546337.0  73835368.0  86290892.0  \n",
       "21    69221493.0  78546337.0  77937911.0  \n",
       "22    84228644.0  69221493.0  85108588.0  \n",
       "23    82759261.0  84228644.0  73835368.0  \n",
       "24    71924711.0  82759261.0  78546337.0  \n",
       "25    70730651.0  71924711.0  69221493.0  \n",
       "26    62023950.0  70730651.0  84228644.0  \n",
       "27    62459404.0  62023950.0  82759261.0  \n",
       "28    73626985.0  62459404.0  71924711.0  \n",
       "29    69694547.0  73626985.0  70730651.0  \n",
       "30    70193502.0  69694547.0  62023950.0  \n",
       "31    65390258.0  70193502.0  62459404.0  \n",
       "32    84103970.0  65390258.0  73626985.0  \n",
       "33    91348734.0  84103970.0  69694547.0  \n",
       "34    89503618.0  91348734.0  70193502.0  \n",
       "35    86753087.0  89503618.0  65390258.0  \n",
       "...          ...         ...         ...  \n",
       "7355         0.0         0.0         0.0  \n",
       "7356         0.0         0.0         0.0  \n",
       "7357         0.0         0.0         0.0  \n",
       "7358         0.0         0.0         0.0  \n",
       "7359         0.0         0.0         0.0  \n",
       "7360         0.0         0.0         0.0  \n",
       "7361         0.0         0.0         0.0  \n",
       "7362         0.0         0.0         0.0  \n",
       "7363         0.0         0.0         0.0  \n",
       "7364         0.0         0.0         0.0  \n",
       "7365         0.0         0.0         0.0  \n",
       "7366         0.0         0.0         0.0  \n",
       "7367         0.0         0.0         0.0  \n",
       "7368         0.0         0.0         0.0  \n",
       "7369         0.0         0.0         0.0  \n",
       "7370         0.0         0.0         0.0  \n",
       "7371         0.0         0.0         0.0  \n",
       "7372         0.0         0.0         0.0  \n",
       "7373         0.0         0.0         0.0  \n",
       "7374         0.0         0.0         0.0  \n",
       "7375         0.0         0.0         0.0  \n",
       "7376         0.0         0.0         0.0  \n",
       "7377         0.0         0.0         0.0  \n",
       "7378         0.0         0.0         0.0  \n",
       "7379         0.0         0.0         0.0  \n",
       "7380         0.0         0.0         0.0  \n",
       "7381         0.0         0.0         0.0  \n",
       "7382         0.0         0.0         0.0  \n",
       "7383         0.0         0.0         0.0  \n",
       "7384         0.0         0.0         0.0  \n",
       "\n",
       "[7379 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Data para modelado\n",
    "file= 'DataframeExportsparamodelado.csv'\n",
    "datos=pd.read_csv(file,\n",
    "                       header=0,\n",
    "                       index_col=0,\n",
    "                       sep=',',\n",
    "                       parse_dates=False,\n",
    "                       skip_blank_lines=True,\n",
    "                 encoding='latin-1')\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import Data variables de entrada \n",
    "file= 'DFExportsvarentrada.csv'\n",
    "variables_entrada=pd.read_csv(file,\n",
    "                       header=0,\n",
    "                       index_col=0,\n",
    "                       sep=',',\n",
    "                       parse_dates=False,\n",
    "                       skip_blank_lines=True,\n",
    "                 encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para facilitar el modelado de random forest\n",
    "def modelfit(algorithm,datos,tipo,final_lags,printFeatureImportance=True): \n",
    "        \n",
    "    #dividir train, y tst\n",
    "    data_train=datos.query('(Año==2016 & Mes<8) or (Año<2016)').reset_index(drop=True) #desde Julio 2000 a Julio 2016\n",
    "    data_test=datos.query('Año == 2017 or (Año==2016 & Mes>7)').reset_index(drop=True) #ultimos 12 meses(agosto 2016 a julio 2017)\n",
    "    \n",
    "    if tipo==1:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,14:15]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,14:15]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "        \n",
    "    else:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,4:5]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,4:5]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "                  \n",
    "    #Fit the algorithm on the training data\n",
    "    algorithm.fit(X_train,Y_train)\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        dtest_predictions = algorithm.predict(X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].reset_index(drop=True)) #hacer predicciones\n",
    "        #mes con mes \n",
    "\n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    dtest_predictions = algorithm.predict(X_test[(X_test.Año==2017) & (X_test.Mes==7)].reset_index(drop=True)) \n",
    "    resultados.Prediccion[t*35:]=dtest_predictions\n",
    "        \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Test mean_squared_error : %.4g\" % mean_squared_error(Y_test.values, resultados.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "           \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        predictors=list(X_train.columns.values)\n",
    "        feat_imp = pd.Series(algorithm.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "    return resultados,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:25: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:461: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Test mean_squared_error : 4.439e+17\n",
      "Test error de la suma total de todas las predicciones : 0.09273\n",
      "Test error del monto de cada prediccion(420) : 1.711\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAFPCAYAAABEeRneAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngEoCiIgYUEkAERED6v8DZFfykMRYrX8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8082190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#crear modelo y mandar llamar funcion para modelado(random forest)\n",
    "tipo=3\n",
    "final_lags=[1,2,3,6] #rezagos significativos \n",
    "gbm0 = RandomForestRegressor(random_state=1) # default parameters, gini by dedault \n",
    "resultados_fun_rf,Y_test_rf=modelfit(gbm0,datos,tipo,final_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exportar vector de datos reales mensuales para comparación de predicciones anuales convertidas a mensuales:\n",
    "Y_test_rf.to_csv('reales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para modelar suppor vector regression\n",
    "\n",
    "def support_vector_m(datos,tipo,final_lags,pol_degree,c,g): \n",
    "    \n",
    "    #dividir train y tst:\n",
    "    data_train=datos.query('(Año==2016 & Mes<8) or (Año<2016)').reset_index(drop=True) #desde Julio 2000 a Julio 2016\n",
    "    data_test=datos.query('Año == 2017 or (Año==2016 & Mes>7)').reset_index(drop=True) #ultimos 12 meses(agosto 2016 a julio 2017)\n",
    "    \n",
    "    if tipo==1:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,14:15]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,14:15]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "        \n",
    "    else:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,4:5]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,4:5]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "    \n",
    "    #soft normalization:\n",
    "    \n",
    "    columns_names=list(X_train.columns) #obtener nombres de columnas(variables) para no perderlos\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train) #sacar medias y varianzas del training set para futura normalización\n",
    "    scaler_y=preprocessing.StandardScaler().fit(Y_train) #sacar medias y varianzas de variable respuesta(traning) para normalizar\n",
    "    #predicciones antes del reemplazo en rezagos\n",
    "    \n",
    "    X_train=scaler.transform(X_train) #normalizar training set\n",
    "    X_train=pd.DataFrame(X_train,columns=columns_names) #convertir de array a DataFrame\n",
    "        \n",
    "    X_test_n=scaler.transform(X_test) #normalizar test set\n",
    "    X_test_n=pd.DataFrame(X_test_n,columns=columns_names) #convertir de array a DataFram\n",
    "    \n",
    "    #entrenar modelos con training data set\n",
    "    \n",
    "    svr = SVR(kernel='linear',C=c).fit(X_train,Y_train) # \"C\" is the penalization for error term\n",
    "    svr_poly = SVR(kernel='poly',degree=pol_degree,C=c).fit(X_train,Y_train) \n",
    "    svr_rbf = SVR(kernel='rbf',C=c,gamma=g).fit(X_train,Y_train) #gamma es el coeficiente del kernel/gamma es el ancho del guassiano                                                  \n",
    "        \n",
    "    # hacer predicciones de test data set::\n",
    "    \n",
    "    ########### predicciones con kernel lineal: #################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_lineal=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = svr.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes \n",
    "        dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions_reshape) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_lineal.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = svr.predict(X_test_n.loc[rows_topredict,:])\n",
    "    resultados_lineal.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_lineal.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_lineal.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Linear Kernel\")\n",
    "    print('\\t C: %1.3f' %c)\n",
    "    print (\"Test mean_squared_error Linear Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_lineal.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ########### predicciones con kernel polinomial:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_poly=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = svr_poly.predict(X_test_n.loc[rows_topredict,:]) #hacer predicciones mes con mes\n",
    "        dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions_reshape) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_poly.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    \n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = svr_poly.predict(X_test_n.loc[rows_topredict,:])\n",
    "    resultados_poly.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_poly.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_poly.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Polynomial Kernel\")\n",
    "    print('\\t Grado polinomio %1.3f' %pol_degree)\n",
    "    print('\\t C: %1.3f' %c)\n",
    "    print (\"Test mean_squared_error Polynomial Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_poly.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    \n",
    "    ########### predicciones con kernel Radial:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_rbf=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = svr_rbf.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes \n",
    "        dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions_reshape) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_rbf.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = svr_rbf.predict(X_test_n.loc[rows_topredict,:])\n",
    "    resultados_rbf.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_rbf.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_rbf.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Radial Kernel\")\n",
    "    print('\\t C: %1.3f' %c)\n",
    "    #print('\\t gamma: %1.3f' %g)\n",
    "    print (\"Test mean_squared_error Radial Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_rbf.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ###hacer data frame con los resultados de los 3 tipos de kernels:\n",
    "    \n",
    "    resultados_fun=pd.DataFrame(np.zeros((len(resultados_rbf),3)),columns=['Linear','Polynomial','Radial'])\n",
    "        \n",
    "    resultados_fun.loc[:,'Linear']=resultados_lineal.Prediccion\n",
    "    resultados_fun.loc[:,'Polynomial']=resultados_poly.Prediccion\n",
    "    resultados_fun.loc[:,'Radial']=resultados_rbf.Prediccion\n",
    "    \n",
    "    return   Y_test,resultados_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:216: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return column_or_1d(y, warn=True).astype(np.float64)\n",
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:216: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return column_or_1d(y, warn=True).astype(np.float64)\n",
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:216: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return column_or_1d(y, warn=True).astype(np.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report Linear Kernel\n",
      "\t C: 100.000\n",
      "Test mean_squared_error Linear Kernel: 2.34e+17\n",
      "Test error de la suma total de todas las predicciones : 0.9983\n",
      "Test error del monto de cada prediccion(420) : 0.9997\n",
      "\n",
      "Model Report Polynomial Kernel\n",
      "\t Grado polinomio 3.000\n",
      "\t C: 100.000\n",
      "Test mean_squared_error Polynomial Kernel: 2.345e+17\n",
      "Test error de la suma total de todas las predicciones : 0.9828\n",
      "Test error del monto de cada prediccion(420) : 1.008\n",
      "\n",
      "Model Report Radial Kernel\n",
      "\t C: 100.000\n",
      "Test mean_squared_error Radial Kernel: 2.34e+17\n",
      "Test error de la suma total de todas las predicciones : 0.9996\n",
      "Test error del monto de cada prediccion(420) : 0.9999\n"
     ]
    }
   ],
   "source": [
    "#mandar llamar funcion para modelado de SVR: ####no funcionaron\n",
    "pol_degree=3\n",
    "c=100\n",
    "g='auto'\n",
    "Y_test_svr,resultados_fun_svr=support_vector_m(datos,tipo,final_lags,pol_degree,c,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para modelar regresiones con penalización L1(lasso) y L2(ridge):\n",
    "\n",
    "def regresiones(datos,tipo,final_lags,Alpha,normalizar_l2norm=True): #si normalizar_l2norm es \"true\" se hará normalización subtracting the mean\n",
    "    #and dividing by the l2-norm. else: estandarización con preprocessing, Standardize features by removing the mean and scaling to unit variance\n",
    "    \n",
    "    #dividir train,cross y tst:\n",
    "    data_train=datos.query('(Año==2016 & Mes<8) or (Año<2016)').reset_index(drop=True) #desde Julio 2000 a Julio 2016\n",
    "    data_test=datos.query('Año == 2017 or (Año==2016 & Mes>7)').reset_index(drop=True) #ultimos 12 meses(agosto 2016 a julio 2017)\n",
    "    \n",
    "    if tipo==1:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,14:15]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,14:15]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "        \n",
    "    else:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,4:5]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,4:5]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "        \n",
    "    if normalizar_l2norm:\n",
    "        #scaler para normalizar prediccioens para reemplazo en razagos \n",
    "        scaler_y=preprocessing.StandardScaler().fit(Y_train) #sacar medias y varianzas de variable respuesta(traning) para normalizar\n",
    "        #predicciones antes del reemplazo en rezagos\n",
    "        \n",
    "        #entrenar modelos con training data set\n",
    "        Lasso_reg= Lasso(alpha=Alpha,normalize=True).fit(X_train,Y_train)#alpha :Constant that multiplies the L1 term. Defaults to 1.0\n",
    "        Ridge_reg= Ridge(alpha=Alpha,normalize=True,solver='auto').fit(X_train,Y_train)#alpha :Regularization strength; must be a positive float. \n",
    "        #Regularization improves the conditioning of the problem and reduces the variance of the estimates. \n",
    "        \n",
    "    else:\n",
    "        #soft normalization:\n",
    "    \n",
    "        columns_names=list(X_train.columns) #obtener nombres de columnas(variables) para no perderlos\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train) #sacar medias y varianzas del training set para futura normalización\n",
    "        scaler_y=preprocessing.StandardScaler().fit(Y_train) #sacar medias y varianzas de variable respuesta(traning) para normalizar\n",
    "        #predicciones antes del reemplazo en rezagos\n",
    "    \n",
    "        X_train=scaler.transform(X_train) #normalizar training set\n",
    "        X_train=pd.DataFrame(X_train,columns=columns_names) #convertir de array a DataFrame\n",
    "        \n",
    "        X_test_n=scaler.transform(X_test) #normalizar test set\n",
    "        X_test_n=pd.DataFrame(X_test_n,columns=columns_names) #convertir de array a DataFram\n",
    "    \n",
    "        #entrenar modelos con training data set\n",
    "        Lasso_reg= Lasso(alpha=Alpha,normalize=False).fit(X_train,Y_train)#alpha :Constant that multiplies the L1 term. Defaults to 1.0\n",
    "        Ridge_reg= Ridge(alpha=Alpha,normalize=False,solver='auto').fit(X_train,Y_train)#alpha :Regularization strength; must be a positive float. \n",
    "        #Regularization improves the conditioning of the problem and reduces the variance of the estimates.                                       \n",
    "    \n",
    " \n",
    "    # hacer predicciones de test data set::\n",
    "    \n",
    "    ########### predicciones con regresion Lasso : #################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_lasso=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        \n",
    "        if normalizar_l2norm:\n",
    "            dtest_predictions = Lasso_reg.predict(X_test.loc[rows_topredict,:])#hacer predicciones mes con mes en X_test normal sklearn normaliza\n",
    "            dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "            dtest_predictions_n=scaler_y.transform(dtest_predictions_reshape) #normalizar predicciones para reemplazo en rezagos\n",
    "            dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler\n",
    "        else:\n",
    "            dtest_predictions = Lasso_reg.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes en X_test normalizado\n",
    "            dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "            dtest_predictions_n=scaler_y.transform(dtest_predictions_reshape) #normalizar predicciones para reemplazo en rezagos\n",
    "            dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "                \n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            \n",
    "            if normalizar_l2norm:\n",
    "                X_test.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando datos reales por las predicciones en test \n",
    "            else:\n",
    "                X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando datos reales por las predicciones en test norm\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_lasso.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    if normalizar_l2norm:\n",
    "        dtest_predictions = Lasso_reg.predict(X_test.loc[rows_topredict,:])\n",
    "    else:\n",
    "        dtest_predictions = Lasso_reg.predict(X_test_n.loc[rows_topredict,:])\n",
    "    resultados_lasso.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_lasso.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_lasso.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Lasso\")\n",
    "    print('\\t alpha: %1.3f' %Alpha)\n",
    "    print (\"Test R2(coefficient of determination) Lasso: %.4g\" % r2_score(Y_test.values,resultados_lasso.Prediccion))\n",
    "    print (\"Test mean square error Lasso: %.4g\" % mean_squared_error(Y_test.values,resultados_lasso.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ########### predicciones con regresion Ridge:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_ridge=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        \n",
    "        if normalizar_l2norm:\n",
    "            dtest_predictions = Ridge_reg.predict(X_test.loc[rows_topredict,:])#hacer predicciones mes con mes en X_test normal sklearn normaliza\n",
    "            #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "            dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "            dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler\n",
    "            dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        else:\n",
    "            dtest_predictions = Ridge_reg.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes en X_test normalizado\n",
    "            #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "            dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "            dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler\n",
    "            dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "                \n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            \n",
    "            if normalizar_l2norm:\n",
    "                X_test.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando datos reales por las predicciones en test \n",
    "            else:\n",
    "                X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando datos reales por las predicciones en test norm\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_ridge.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    if normalizar_l2norm:\n",
    "        dtest_predictions = Ridge_reg.predict(X_test.loc[rows_topredict,:])\n",
    "        dtest_predictions= dtest_predictions.reshape(-1,)\n",
    "    else:\n",
    "        dtest_predictions = Ridge_reg.predict(X_test_n.loc[rows_topredict,:])\n",
    "        dtest_predictions= dtest_predictions.reshape(-1,)\n",
    "    resultados_ridge.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_ridge.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_ridge.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Ridge\")\n",
    "    print('\\t alpha: %1.3f' %Alpha)\n",
    "    print (\"Test R2(coefficient of determination) Ridge: %.4g\" % r2_score(Y_test.values,resultados_ridge.Prediccion))\n",
    "    print (\"Test mean square error Ridge: %.4g\" % mean_squared_error(Y_test.values,resultados_ridge.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "            \n",
    "    ###hacer data frame con los resultados de los 2 tipos de penalización:\n",
    "    \n",
    "    resultados_fun=pd.DataFrame(np.zeros((len(resultados_lasso),2)),columns=['Lasso','Ridge'])\n",
    "    \n",
    "    resultados_fun.loc[:,'Lasso']=resultados_lasso.Prediccion\n",
    "    resultados_fun.loc[:,'Ridge']=resultados_ridge.Prediccion\n",
    "    \n",
    "    return   Y_test,resultados_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edu\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:461: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report Lasso\n",
      "\t alpha: 10.000\n",
      "Test R2(coefficient of determination) Lasso: -0.1589\n",
      "Test mean square error Lasso: 2.544e+17\n",
      "Test error de la suma total de todas las predicciones : 0.8617\n",
      "Test error del monto de cada prediccion(420) : 1.079\n",
      "\n",
      "Model Report Ridge\n",
      "\t alpha: 10.000\n",
      "Test R2(coefficient of determination) Ridge: -0.03069\n",
      "Test mean square error Ridge: 2.262e+17\n",
      "Test error de la suma total de todas las predicciones : 0.5211\n",
      "Test error del monto de cada prediccion(420) : 1.295\n"
     ]
    }
   ],
   "source": [
    "#mandar llamar funcion para modelar regresiones: ####no funcionaron\n",
    "        \n",
    "Alpha=10\n",
    "Y_test_regre,resultados_fun_regre=regresiones(datos,tipo,final_lags,Alpha,normalizar_l2norm=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para modelar Regresion Kernel Ridge\n",
    "def kernel_ridge(datos,tipo,final_lags,Alpha,pol_degree,g): \n",
    "    \n",
    "    #dividir train,cross y tst:\n",
    "    data_train=datos.query('(Año==2016 & Mes<8) or (Año<2016)').reset_index(drop=True) #desde Julio 2000 a Julio 2016\n",
    "    data_test=datos.query('Año == 2017 or (Año==2016 & Mes>7)').reset_index(drop=True) #ultimos 12 meses(agosto 2016 a julio 2017)\n",
    "    \n",
    "    if tipo==1:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,14:15]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,14:15]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "        \n",
    "    else:\n",
    "        #separar \"x,y\"\n",
    "        Y_train=data_train.iloc[:,4:5]\n",
    "        X_train=data_train.ix[:, data_train.columns != 'Monto']\n",
    "\n",
    "        Y_test=data_test.iloc[:,4:5]\n",
    "        X_test=data_test.ix[:, data_test.columns != 'Monto']\n",
    "    \n",
    "    #soft normalization:\n",
    "    \n",
    "    columns_names=list(X_train.columns) #obtener nombres de columnas(variables) para no perderlos\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train) #sacar medias y varianzas del training set para futura normalización\n",
    "    scaler_y=preprocessing.StandardScaler().fit(Y_train) #sacar medias y varianzas de variable respuesta(traning) para normalizar\n",
    "    #predicciones antes del reemplazo en rezagos\n",
    "    \n",
    "    X_train=scaler.transform(X_train) #normalizar training set\n",
    "    X_train=pd.DataFrame(X_train,columns=columns_names) #convertir de array a DataFrame\n",
    "        \n",
    "    X_test_n=scaler.transform(X_test) #normalizar test set\n",
    "    X_test_n=pd.DataFrame(X_test_n,columns=columns_names) #convertir de array a DataFram\n",
    "    \n",
    "    #entrenar modelos con training data set\n",
    "    \n",
    "    Kridge_linear = KernelRidge(kernel='linear',alpha=Alpha).fit(X_train,Y_train)#Small positive values of alpha improve the conditioning \n",
    "    #of the problem and reduce the variance of the estimates\n",
    "    Kridge_poly = KernelRidge(kernel='poly',degree=pol_degree,alpha=Alpha,gamma=g).fit(X_train,Y_train) \n",
    "    Kridge_rbf = KernelRidge(kernel='rbf',alpha=Alpha,gamma=g).fit(X_train,Y_train) #gamma es el coeficiente del kernel/gamma es el ancho del guassiano                                                  \n",
    "    Kridge_laplacian= KernelRidge(kernel='laplacian',alpha=Alpha,gamma=g).fit(X_train,Y_train) # \"C\" is the penalization for error term\n",
    "    \n",
    "    # hacer predicciones de test data set::\n",
    "    \n",
    "    ########### predicciones con kernel lineal: #################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_lineal=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = Kridge_linear.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes \n",
    "        #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_lineal.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = Kridge_linear.predict(X_test_n.loc[rows_topredict,:])\n",
    "    dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "    resultados_lineal.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_lineal.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_lineal.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Linear Kernel\")\n",
    "    print('\\t Alpha: %1.3f' %Alpha)\n",
    "    print (\"Test mean_squared_error Linear Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_lineal.Prediccion))\n",
    "    print (\"Test R2(coefficient of determination) Linear Kernel: %.4g\" % r2_score(Y_test.values,resultados_lineal.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ########### predicciones con kernel polinomial:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_poly=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = Kridge_poly.predict(X_test_n.loc[rows_topredict,:]) #hacer predicciones mes con mes\n",
    "        #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler\n",
    "        dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_poly.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    \n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = Kridge_poly.predict(X_test_n.loc[rows_topredict,:])\n",
    "    dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "    resultados_poly.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_poly.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_poly.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Polynomial Kernel\")\n",
    "    print('\\t Grado polinomio %1.3f' %pol_degree)\n",
    "    print('\\t Alpha: %1.3f' %Alpha)\n",
    "    print('\\t Gamma: %1.3f' %g)\n",
    "    print (\"Test mean_squared_error Polynomial Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_poly.Prediccion))\n",
    "    print (\"Test R2(coefficient of determination) Polynomial Kernel: %.4g\" % r2_score(Y_test.values,resultados_poly.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    \n",
    "    ########### predicciones con kernel Radial:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_rbf=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = Kridge_rbf.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes \n",
    "        #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_rbf.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = Kridge_rbf.predict(X_test_n.loc[rows_topredict,:])\n",
    "    dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "    resultados_rbf.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_rbf.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_rbf.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Radial Kernel\")\n",
    "    print('\\t Alpha: %1.3f' %Alpha)\n",
    "    print('\\t gamma: %1.3f' %g)\n",
    "    print (\"Test mean_squared_error Radial Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_rbf.Prediccion))\n",
    "    print (\"Test R2(coefficient of determination) Radial Kernel: %.4g\" % r2_score(Y_test.values,resultados_rbf.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ########### predicciones con kernel Laplacian:#################\n",
    "    \n",
    "    #Predict test set:  #hace predicción mes con mes y las va sustituyendo en el conjunto X_test para las siguientes predicciones \n",
    " \n",
    "    total_lags=[1,2,3,4,5,6,7,8,9,10,11]#todos los rezagos \n",
    "    meses_apredecir=[8,9,10,11,12,1,2,3,4,5,6] #meses a predecir en el ciclo for(11 meses) el 12° mes se predice por separado al último\n",
    "    aux=0 #se utiliza para ir disminuyendo el umbral para saber si el mes a reemplazar pertenece a 2016 o 2017 a partir del mes 12 2016 todos los \n",
    "    #reemplazos se harán en el año 2017\n",
    "    resultados_lapla=pd.DataFrame(data=np.zeros((420,1)),columns=['Prediccion']) #Data frame donde se guardarán los pronósticos \n",
    "    \n",
    "    for t,mes_predic in zip(range(1,12,1),meses_apredecir):  ########aureca , es el bueno por fin!\n",
    "        if t<6:\n",
    "            año_predic=2016\n",
    "        else:\n",
    "            año_predic=2017\n",
    "        rows_topredict=X_test[(X_test.Año==año_predic) & (X_test.Mes==mes_predic)].index.values #indices de filas de las que se harán predicciones\n",
    "        dtest_predictions = Kridge_laplacian.predict(X_test_n.loc[rows_topredict,:])#hacer predicciones mes con mes \n",
    "        #dtest_predictions_reshape=dtest_predictions.reshape(-1, 1) # reshape para el scaler \"arrojaba warnings\"\n",
    "        dtest_predictions_n=scaler_y.transform(dtest_predictions) #normalizar predicciones para reemplazo en rezagos\n",
    "        dtest_predictions_n=dtest_predictions_n.reshape(-1,) #deshacer efecto del reshape que se usó para el scaler \n",
    "        dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "        \n",
    "        lags=set(total_lags).intersection(final_lags) #para encontrar los que coinciden entre total_lags y final_lags \n",
    "        lags=list(lags) #rezagos a reemplazar en cada iteración de predicciones \n",
    "\n",
    "        index=list(range(1,13-t,1))\n",
    "        meses=pd.DataFrame([9,10,11,12,1,2,3,4,5,6,7],columns=['Meses']) #todos los meses en los que se reemplazarían las predicciones \n",
    "        meses=meses.iloc[t-1:,:]\n",
    "        meses['index']=index\n",
    "        meses=meses.set_index('index')\n",
    "        meses=meses.loc[lags,:]\n",
    "        meses=meses.Meses.values.tolist() #meses en los que se reemplaza las predicciones dados los rezagos significativos (final_lags)\n",
    "\n",
    "        total_lags=total_lags[:-1] #para ir eliminando el último elemento de la lista \"total_lags\"\n",
    "\n",
    "        for i,j in zip(lags,meses): #ciclo para ir reemplazando las predicciones ,\"i\" sería el contador para rezagos y \"j\" contador para meses a reemplazar\n",
    "            if i<(5-aux):# para separar 2016 y 2017\n",
    "                año_replace=2016\n",
    "            else:\n",
    "                año_replace=2017\n",
    "            rows_toreplace=X_test[(X_test.Año==año_replace) & (X_test.Mes==j)].index.values # filas a reemplazar con predicciones\n",
    "            X_test_n.loc[rows_toreplace,'t-'+str(i)]=dtest_predictions_n #reemplazando los datos reales por las predicciones en conjunto test\n",
    "        aux=aux+1 \n",
    "        #print(dtest_predictions)\n",
    "        resultados_lapla.Prediccion[(t-1)*35:t*35]=dtest_predictions #35 porque se hacen de 35 en 35 predicciones \n",
    "\n",
    "    ##predicciones del último mes( Julio 2017)\n",
    "    rows_topredict=X_test[(X_test.Año==2017) & (X_test.Mes==7)].index.values #indices de filas de las que se harán predicciones\n",
    "    dtest_predictions = Kridge_laplacian.predict(X_test_n.loc[rows_topredict,:])\n",
    "    dtest_predictions=dtest_predictions.reshape(-1,)\n",
    "    resultados_lapla.Prediccion[t*35:]=dtest_predictions\n",
    "    \n",
    "    #errores personalizados para test set:\n",
    "    diferencias_abs=pd.DataFrame(abs(Y_test.values-resultados_lapla.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(resultados_lapla.Prediccion)\n",
    "    sum_reales=sum(Y_test.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report Laplacian Kernel\")\n",
    "    print('\\t Alpha: %1.3f' %Alpha)\n",
    "    print('\\t gamma: %1.3f' %g)\n",
    "    print (\"Test mean_squared_error Laplacian Kernel: %.4g\" % mean_squared_error(Y_test.values,resultados_lapla.Prediccion))\n",
    "    print (\"Test R2(coefficient of determination) Laplacian Kernel: %.4g\" % r2_score(Y_test.values,resultados_lapla.Prediccion))\n",
    "    print (\"Test error de la suma total de todas las predicciones : %.4g\" %porcentaje_error)\n",
    "    print (\"Test error del monto de cada prediccion(420) : %.4g\" %porcentaje_error_abs)\n",
    "    \n",
    "    ###hacer data frame con los resultados de los 4 tipos de kernels:\n",
    "    \n",
    "    resultados_fun=pd.DataFrame(np.zeros((len(resultados_rbf),4)),columns=['Linear','Polynomial','Radial','Laplacian'])\n",
    "        \n",
    "    resultados_fun.loc[:,'Linear']=resultados_lineal.Prediccion\n",
    "    resultados_fun.loc[:,'Polynomial']=resultados_poly.Prediccion\n",
    "    resultados_fun.loc[:,'Radial']=resultados_rbf.Prediccion\n",
    "    resultados_fun.loc[:,'Laplacian']=resultados_lapla.Prediccion\n",
    "    \n",
    "    return   Y_test,resultados_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report Linear Kernel\n",
      "\t Alpha: 1.000\n",
      "Test mean_squared_error Linear Kernel: 5.383e+17\n",
      "Test R2(coefficient of determination) Linear Kernel: -1.452\n",
      "Test error de la suma total de todas las predicciones : 2.524\n",
      "Test error del monto de cada prediccion(420) : 3.619\n",
      "\n",
      "Model Report Polynomial Kernel\n",
      "\t Grado polinomio 3.000\n",
      "\t Alpha: 1.000\n",
      "\t Gamma: 0.002\n",
      "Test mean_squared_error Polynomial Kernel: 4.539e+17\n",
      "Test R2(coefficient of determination) Polynomial Kernel: -1.068\n",
      "Test error de la suma total de todas las predicciones : 0.005256\n",
      "Test error del monto de cada prediccion(420) : 1.778\n",
      "\n",
      "Model Report Radial Kernel\n",
      "\t Alpha: 1.000\n",
      "\t gamma: 0.002\n",
      "Test mean_squared_error Radial Kernel: 3.769e+17\n",
      "Test R2(coefficient of determination) Radial Kernel: -0.717\n",
      "Test error de la suma total de todas las predicciones : 0.1276\n",
      "Test error del monto de cada prediccion(420) : 1.635\n",
      "\n",
      "Model Report Laplacian Kernel\n",
      "\t Alpha: 1.000\n",
      "\t gamma: 0.002\n",
      "Test mean_squared_error Laplacian Kernel: 3.295e+17\n",
      "Test R2(coefficient of determination) Laplacian Kernel: -0.501\n",
      "Test error de la suma total de todas las predicciones : 0.1166\n",
      "Test error del monto de cada prediccion(420) : 1.623\n"
     ]
    }
   ],
   "source": [
    "#mandar llamar funcion para modelado con regresión Kernel Ridge:\n",
    "        \n",
    "Alpha=1\n",
    "pol_degree=3\n",
    "g=.002\n",
    "Y_test_KR,resultados_fun_KR=kernel_ridge(datos,tipo,final_lags,Alpha,pol_degree,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para errores por segmentos(pais,industria,año,mes)\n",
    "\n",
    "def errores(variables_entrada,reales,predicciones,col,pais,industria,año,mes,s_pais=True,s_industria=True,s_año=True,s_mes=True):\n",
    "    # totales:\n",
    "    \n",
    "    diferencias_abs=pd.DataFrame(abs(reales.Monto.values-predicciones.iloc[:,col].values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(predicciones.iloc[:,col])\n",
    "    sum_reales=sum(reales.Monto)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion \n",
    "    \n",
    "    d={'Total_Real':sum_reales,'Total_Predicciones':sum_predicciones}\n",
    "    pd_plot=pd.DataFrame(d,index=['Total'])\n",
    "        \n",
    "    titulo='Montos totales anuales'\n",
    "    print(\"Monto total Anual:\")\n",
    "    print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "    print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "    print(pd_plot.plot(title=titulo,kind='bar',figsize=(14,8)))\n",
    "    print(pd_plot)\n",
    "    \n",
    "    if s_pais & s_industria & s_año & s_mes:\n",
    "        \n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Pais_comprador==pais[0]) & (variables_entrada.Industria==industria[0])\n",
    "                                & (variables_entrada.Año==año) & (variables_entrada.Mes==mes)].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion \n",
    "        \n",
    "        print(pais,industria,año,mes,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "    if s_pais & s_industria & s_año:\n",
    "        \n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Pais_comprador==pais[0]) & (variables_entrada.Industria==industria[0])\n",
    "                                & (variables_entrada.Año==año)].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion \n",
    "        \n",
    "        print(pais,industria,año,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "        \n",
    "    if s_pais & s_industria:\n",
    "        \n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Pais_comprador==pais[0]) & (variables_entrada.Industria==industria[0])].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "        print(pais,industria,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "    if s_pais:\n",
    "        \n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Pais_comprador==pais[0])].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "        \n",
    "        print(pais,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "        \n",
    "    if s_industria:\n",
    "\n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Industria==industria[0])].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "\n",
    "        print(industria,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)\n",
    "        \n",
    "    if s_año & s_mes:\n",
    "\n",
    "        rows_toreplace=variables_entrada[(variables_entrada.Año==año) & (variables_entrada.Mes==mes)].index.values \n",
    "        real=reales.iloc[rows_toreplace,:]\n",
    "        prediccion=predicciones.iloc[rows_toreplace,col]\n",
    "        prediccion=pd.DataFrame(prediccion.values,columns=['Prediccion'])\n",
    "\n",
    "        diferencias_abs=pd.DataFrame(abs(real.values-prediccion.values),columns =['Diferencias'])\n",
    "        sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "        sum_predicciones=sum(prediccion.Prediccion)\n",
    "        sum_reales=sum(real.Monto)\n",
    "        diferencia_total=sum_reales-sum_predicciones\n",
    "\n",
    "        porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "        porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion\n",
    "\n",
    "        print(\"Año-\",año,\"mes-\",mes,\":\")\n",
    "        print(\"Error de la suma total de predicciones: %.4g\" % porcentaje_error)\n",
    "        print(\"Error de cada prediccion : %.4g\" % porcentaje_error_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monto total Anual:\n",
      "Error de la suma total de predicciones: 0.005256\n",
      "Error de cada prediccion : 1.778\n",
      "Axes(0.125,0.125;0.775x0.775)\n",
      "       Total_Predicciones    Total_Real\n",
      "Total        5.039346e+10  5.065973e+10\n",
      "['América del Norte'] ['Industrias Manufactureras'] 2016 8 :\n",
      "Error de la suma total de predicciones: 0.9767\n",
      "Error de cada prediccion : 0.9767\n",
      "['América del Norte'] ['Industrias Manufactureras'] 2016 :\n",
      "Error de la suma total de predicciones: 0.9953\n",
      "Error de cada prediccion : 0.9953\n",
      "['América del Norte'] ['Industrias Manufactureras'] :\n",
      "Error de la suma total de predicciones: 0.892\n",
      "Error de cada prediccion : 0.9428\n",
      "['América del Norte'] :\n",
      "Error de la suma total de predicciones: 0.7623\n",
      "Error de cada prediccion : 1.063\n",
      "['Industrias Manufactureras'] :\n",
      "Error de la suma total de predicciones: 0.7431\n",
      "Error de cada prediccion : 1.044\n",
      "Año- 2016 mes- 8 :\n",
      "Error de la suma total de predicciones: 0.0869\n",
      "Error de cada prediccion : 1.826\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyUAAAH4CAYAAABUq0JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAEAAJoSJQAAQFOiBAAAaEqUAAAATf3/TB1uWUPPEskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9889d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mandar llamar funcion de errores\n",
    "\n",
    "#lista paises: América Latina,Asia,Europa,Africa,Oceanía,América del Norte,otros\n",
    "#lista industrias: Industrias Manufactureras//Agricultura y Silvicultura//Servicios y Productos no Clasificados \n",
    "                ## Industrias Extractivas//Ganadería, Apicultura, Caza, Pesca\n",
    "#lista años: 2016, 2017\n",
    "#lista meses: 8,9,10,11,12,1,2,3,4,5,6,7\n",
    "col=1\n",
    "pais=['América del Norte']\n",
    "industria=['Industrias Manufactureras']\n",
    "año=2016\n",
    "mes=8\n",
    "errores(variables_entrada,Y_test_rf,resultados_fun_KR,col,pais,industria,año,mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para crear data frame de datos reales y predicciones agrupadas por año y mes(montos mensuales totales)\n",
    "def DF_realesvspredicciones(variables_entrada,reales,predicciones,col):\n",
    "    kr=predicciones.iloc[:,col]\n",
    "    kr=kr.reshape(-1,)\n",
    "    ky=reales.values\n",
    "    ky=ky.reshape(-1,)\n",
    "    km=variables_entrada.Mes\n",
    "    km=km.reshape(-1,)\n",
    "    kyear=variables_entrada.Año\n",
    "    kyear=kyear.reshape(-1,)\n",
    "    \n",
    "    d={'Año':kyear,'Mes':km,'Reales':ky,'Predicciones':kr}\n",
    "    daataa=pd.DataFrame(d)\n",
    "    dataagrupada=daataa.groupby(['Año','Mes'],as_index=False,sort=False).sum()\n",
    "    return dataagrupada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Año</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Predicciones</th>\n",
       "      <th>Reales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4.158391e+09</td>\n",
       "      <td>3.825921e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4.116550e+09</td>\n",
       "      <td>4.539046e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.140828e+09</td>\n",
       "      <td>4.277424e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.134623e+09</td>\n",
       "      <td>4.165294e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4.145648e+09</td>\n",
       "      <td>3.816237e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.151279e+09</td>\n",
       "      <td>3.888871e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.207962e+09</td>\n",
       "      <td>4.147170e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.247184e+09</td>\n",
       "      <td>4.383166e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.241656e+09</td>\n",
       "      <td>4.289357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.353970e+09</td>\n",
       "      <td>4.770543e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.263097e+09</td>\n",
       "      <td>4.407871e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.232271e+09</td>\n",
       "      <td>4.148828e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Año  Mes  Predicciones        Reales\n",
       "0   2016.0    8  4.158391e+09  3.825921e+09\n",
       "1   2016.0    9  4.116550e+09  4.539046e+09\n",
       "2   2016.0   10  4.140828e+09  4.277424e+09\n",
       "3   2016.0   11  4.134623e+09  4.165294e+09\n",
       "4   2016.0   12  4.145648e+09  3.816237e+09\n",
       "5   2017.0    1  4.151279e+09  3.888871e+09\n",
       "6   2017.0    2  4.207962e+09  4.147170e+09\n",
       "7   2017.0    3  4.247184e+09  4.383166e+09\n",
       "8   2017.0    4  4.241656e+09  4.289357e+09\n",
       "9   2017.0    5  4.353970e+09  4.770543e+09\n",
       "10  2017.0    6  4.263097e+09  4.407871e+09\n",
       "11  2017.0    7  4.232271e+09  4.148828e+09"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mandar llamar funcion para dataframe reales vs predicciones (montos mensuales totales):\n",
    "col=1 # \n",
    "reales_predicciones=DF_realesvspredicciones(variables_entrada,Y_test_rf,resultados_fun_KR,col)\n",
    "reales_predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para errores mensuales:\n",
    "\n",
    "def errores_mensuales(data):\n",
    "    \n",
    "    # errores mensuales:\n",
    "    \n",
    "    diferencias_abs=pd.DataFrame(abs(data.Reales.values-data.Predicciones.values),columns =['Diferencias'])\n",
    "    sum_diferencias_abs=sum(diferencias_abs.Diferencias)\n",
    "    sum_predicciones=sum(data.Predicciones)\n",
    "    sum_reales=sum(data.Reales)\n",
    "    diferencia_total=sum_reales-sum_predicciones\n",
    "    \n",
    "    errores_mensuales=pd.DataFrame(diferencias_abs.Diferencias.values/reales_predicciones.Reales.values,columns=['error_mensual'])\n",
    "    errores_mensuales=errores_mensuales.join(data.Mes)\n",
    "    porcentaje_error=abs(diferencia_total/sum_reales) #error de la suma total de todas las predicciones\n",
    "    porcentaje_error_abs=sum_diferencias_abs/sum_reales #error del monto de cada prediccion \n",
    "\n",
    "    print(\"Error de la suma total de predicciones mensuales: %.4g\" % porcentaje_error)\n",
    "    print(\"Error de cada prediccion mensual: %.4g\" % porcentaje_error_abs)\n",
    "    \n",
    "    return errores_mensuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error de la suma total de predicciones mensuales: 0.005256\n",
      "Error de cada prediccion mensual: 0.04744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_mensual</th>\n",
       "      <th>Mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086899</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093080</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031934</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007364</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086318</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.067477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014659</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031024</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011121</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.087322</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.032845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020112</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    error_mensual  Mes\n",
       "0        0.086899    8\n",
       "1        0.093080    9\n",
       "2        0.031934   10\n",
       "3        0.007364   11\n",
       "4        0.086318   12\n",
       "5        0.067477    1\n",
       "6        0.014659    2\n",
       "7        0.031024    3\n",
       "8        0.011121    4\n",
       "9        0.087322    5\n",
       "10       0.032845    6\n",
       "11       0.020112    7"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mandar llamar funcion de errores mensuales:\n",
    "df_errores_mensuales=errores_mensuales(reales_predicciones)\n",
    "df_errores_mensuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exportar graficas errores mensuales para comparativo con errores mensuales desde datos anuales:\n",
    "\n",
    "df_errores_mensuales.to_csv('df_errores_mensuales_exp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#funcion para graficar montos mensuales totales reales vs predecidos:\n",
    "\n",
    "def graficar_monto_pred(data):\n",
    "    resultados2=pd.DataFrame(data[['Reales','Predicciones']]).reset_index(drop=True)\n",
    "    print(resultados2.plot(title='Datos reales vs prediccion',figsize=(14,8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axes(0.125,0.125;0.775x0.775)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy8AAAHpCAYAAACLAwOwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x963cf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graficar_monto_pred(reales_predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
